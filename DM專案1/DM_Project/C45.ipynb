{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5決策樹實現_Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, x, y, attribute_list, node_type):\n",
    "        self.data = x\n",
    "        self.labels = y\n",
    "        self.attributes_list = attribute_list\n",
    "        self.best_attribute = None\n",
    "        self.split_criterion = None\n",
    "        self.split_up_down = None\n",
    "        self.node_type = node_type\n",
    "        self.leaf_label = None\n",
    "        self.depth = 0\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.depth < other.depth\n",
    "\n",
    "    def predict_leaf_class(self):\n",
    "        \"\"\"\n",
    "            Computes the frequency of classes in partition D, output the leaf node label predicted class\n",
    "        :return: pred_class\n",
    "        \"\"\"\n",
    "        # takes frequency of classes in D to determine the majority class to set as output leaf label\n",
    "        freq_classes = collections.Counter(self.labels)  # [4]\n",
    "        pred_class = max(freq_classes, key=freq_classes.get)\n",
    "        self.leaf_label = pred_class\n",
    "        return pred_class\n",
    "\n",
    "    def print_node(self):\n",
    "        \"\"\"\n",
    "            Print node values\n",
    "        \"\"\"\n",
    "        print('best att-', self.best_attribute, 'split_crit-', self.split_up_down, self.split_criterion, 'type-',\n",
    "              self.node_type, 'depth-',\n",
    "              self.depth, 'class label-', self.leaf_label)\n",
    "\n",
    "    def copy(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class C45Tree:\n",
    "    def __init__(self, attributes, data):\n",
    "        self.tree_nodes = []\n",
    "        self.depth = 0\n",
    "        self.num_leaves = 0\n",
    "        self.root_node = None\n",
    "        self.attributes = attributes[:-1]\n",
    "        self.dataset = data\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "            Helper function to grow tree recursively, creates root node for the tree and initializes the recursion for\n",
    "            training the tree.\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        \"\"\"\n",
    "        # create root node, put data partition in node\n",
    "        self.root_node = Node(x_train, y_train, self.attributes, 'root')\n",
    "        self.tree_nodes.append(self.root_node)\n",
    "        # call grow_tree with root node as base\n",
    "        self.grow_tree(self.root_node, self.attributes, (x_train, y_train))\n",
    "\n",
    "    def grow_tree(self, prev_node, attribute_list, D):\n",
    "        \"\"\"\n",
    "            Uses C4.5 decision tree algorithm to grow a tree during training, based on pseudocode from [1].\n",
    "        :param attribute_list:\n",
    "        :param D:\n",
    "        :param prev_node:\n",
    "        :return: N, the new node\n",
    "        \"\"\"\n",
    "        if prev_node is not None and prev_node.parent is not None:\n",
    "            if prev_node not in prev_node.parent.children:\n",
    "                prev_node.parent.children.append(prev_node)\n",
    "\n",
    "        # check for termination cases\n",
    "        # check if all tuples in D are in the same class\n",
    "        if self.check_same_class_labels(D[1]):\n",
    "            N = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "            N.depth = prev_node.depth + 1\n",
    "            N.predict_leaf_class()  # determine the class of the leaf\n",
    "            N.best_attribute = str(prev_node.best_attribute)\n",
    "            N.split_up_down = prev_node.split_up_down\n",
    "            N.split_criterion = prev_node.split_criterion\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            N.parent = prev_node\n",
    "            return N\n",
    "\n",
    "        # check if attribute list is empty, do majority voting on class\n",
    "        if not attribute_list:\n",
    "            N = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "            N.depth = prev_node.depth + 1\n",
    "            N.predict_leaf_class()  # determine the class of the leaf\n",
    "            N.best_attribute = str(prev_node.best_attribute)\n",
    "            N.split_criterion = prev_node.split_criterion\n",
    "            N.split_up_down = prev_node.split_up_down\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            N.parent = prev_node\n",
    "            return N\n",
    "\n",
    "        # create new node\n",
    "        N = Node(D[0], D[1], attribute_list, 'node')\n",
    "        N.depth = prev_node.depth + 1\n",
    "        N.parent = prev_node\n",
    "        # conduct attribute selection method, label node with the criterion\n",
    "        best_attribute, crit_split_val = self.attribute_selection_method(D, attribute_list)\n",
    "\n",
    "        N.best_attribute = best_attribute  # label node with best attribute\n",
    "        N.split_criterion = crit_split_val  # for discrete\n",
    "        if best_attribute == '':\n",
    "            # early stop\n",
    "            N.best_attribute = str(best_attribute)\n",
    "            N.split_up_down = None\n",
    "            N.node_type = 'leaf'\n",
    "            N.data = prev_node.data\n",
    "            N.labels = prev_node.labels\n",
    "            N.predict_leaf_class()\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "            return N\n",
    "\n",
    "        # remove split attribute from attribute list\n",
    "        if best_attribute in attribute_list:\n",
    "            attribute_list.remove(best_attribute)\n",
    "\n",
    "        # check if attribute is discrete NOTE THIS LINE NEEDS TO BE MODIFIED FOR DIFFERENT DATASET\n",
    "        if len(self.dataset[\n",
    "                   best_attribute].unique()) > 5:  # max 5 discrete categories in attributes from Thyroid set\n",
    "            # continuous, divide up data at mid point of the values ai + ai1/2\n",
    "            l_part, r_part, split_val = self.continuous_attribute_data_partition(D, best_attribute)\n",
    "            N.split_criterion = split_val\n",
    "            N.split_up_down = 'UP'\n",
    "            l_child = self.grow_tree(N, attribute_list, l_part)  # upper -> att_val > split_val\n",
    "            N_V = Node(D[0], D[1], attribute_list, 'node')\n",
    "            N_V.depth = N.depth\n",
    "            N_V.best_attribute = best_attribute\n",
    "            N_V.split_criterion = split_val\n",
    "            N_V.parent = prev_node\n",
    "            N_V.split_up_down = 'DOWN'\n",
    "            r_child = self.grow_tree(N_V, attribute_list, r_part)  # lower -> att_val <= split_val\n",
    "            N.children.append(l_child)\n",
    "            N_V.children.append(r_child)\n",
    "            N.parent = prev_node\n",
    "            self.tree_nodes.append(N)\n",
    "            self.tree_nodes.append(N_V)\n",
    "            prev_node.children.append(N)\n",
    "            prev_node.children.append(N_V)\n",
    "            return N\n",
    "        else:\n",
    "            # discrete, partition based on unique values of attribute to create nodes for recursion\n",
    "            vals = self.dataset[best_attribute].unique()  # D[0][best_attribute].unique()\n",
    "            for v in list(vals):\n",
    "                data_part = self.partition_data(D, best_attribute, v)\n",
    "\n",
    "                if not data_part:  # TOGGLED TO EMPTY CAUSES 2 LEAVES ONLY TO BE MADE ** check this\n",
    "                    # majority class leaf node computed of D\n",
    "                    L = Node(D[0], D[1], attribute_list, 'leaf')\n",
    "                    L.depth = N.depth + 1\n",
    "                    L.best_attribute = best_attribute\n",
    "                    L.split_criterion = v\n",
    "                    L.predict_leaf_class()  # determine the class of the leaf\n",
    "                    self.tree_nodes.append(L)\n",
    "                    N.children.append(L)\n",
    "                    L.parent = N\n",
    "                else:\n",
    "                    # recursion\n",
    "                    N_V = Node(D[0], D[1], attribute_list, 'node')\n",
    "                    N_V.depth = N.depth\n",
    "                    N_V.best_attribute = best_attribute\n",
    "                    N_V.split_criterion = v\n",
    "                    N_V.parent = prev_node\n",
    "                    N_V.parent.children.append(N_V)\n",
    "                    child = self.grow_tree(N_V, attribute_list, data_part)\n",
    "\n",
    "        if N not in self.tree_nodes:\n",
    "            self.tree_nodes.append(N)\n",
    "            prev_node.children.append(N)\n",
    "        return N\n",
    "\n",
    "    def continuous_attribute_data_partition(self, D, attribute):\n",
    "        \"\"\"\n",
    "            Creates data partitions (left and right) for continuous attributes, computing the mid point that\n",
    "            enables the best information gain ratio to be calculated from the partition.\n",
    "        :param D:\n",
    "        :param attribute:\n",
    "        :return: l_part, r_part, split_val\n",
    "        \"\"\"\n",
    "        # sort the data, find the value that will gain the max info gain ratio\n",
    "        data = D[0].sort_values(by=[attribute])\n",
    "        split_val = 0\n",
    "        best_igr = 0\n",
    "        l_part = []\n",
    "        r_part = []\n",
    "\n",
    "        for i in range(0, len(data) - 1):\n",
    "            mid_point = (float(data.iloc[i][attribute]) + float(data.iloc[i + 1][attribute])) / 2\n",
    "            left_d = D[0].loc[pd.to_numeric(D[0][attribute]) > mid_point]\n",
    "            left_idx = D[0].index[pd.to_numeric(D[0][attribute]) > mid_point]\n",
    "            left_y = D[1].loc[left_idx]\n",
    "            right_d = D[0].loc[pd.to_numeric(D[0][attribute]) <= mid_point]\n",
    "            right_idx = D[0].index[pd.to_numeric(D[0][attribute]) <= mid_point]\n",
    "            right_y = D[1].loc[right_idx]\n",
    "            igr = self.compute_info_gain_ratio_continuous(D, left_y, right_y)\n",
    "\n",
    "            if igr >= best_igr:\n",
    "                best_igr = igr\n",
    "                split_val = mid_point\n",
    "                l_part = (left_d, left_y)\n",
    "                r_part = (right_d, right_y)\n",
    "\n",
    "        return l_part, r_part, split_val\n",
    "\n",
    "    def compute_info_gain_ratio_continuous(self, D, left_y, right_y):\n",
    "        \"\"\"\n",
    "            Computes the information gain ratio for a continuous attribute partition\n",
    "        :return info_gain_ratio\n",
    "        \"\"\"\n",
    "        l_y = left_y\n",
    "        r_y = right_y\n",
    "\n",
    "        dataset_entropy = self.data_entropy(D[1])\n",
    "        l_part_entropy = self.data_entropy(l_y)\n",
    "        l_p_j = float(len(l_y) / len(D))\n",
    "        l_ent = l_p_j * l_part_entropy\n",
    "        r_part_entropy = self.data_entropy(r_y)\n",
    "        r_p_j = float(len(r_y) / len(D))\n",
    "        r_ent = r_p_j * r_part_entropy\n",
    "\n",
    "        split_info = - self.split_info(l_p_j) - self.split_info(r_p_j)\n",
    "        att_ent = l_ent + r_ent\n",
    "\n",
    "        if split_info == 0:  # prevent division by zero for ratio\n",
    "            return 0\n",
    "        else:\n",
    "            info_gain = self.information_gain(dataset_entropy, att_ent)\n",
    "            info_gain_ratio = self.information_gain_ratio(info_gain,\n",
    "                                                          split_info)\n",
    "        return info_gain_ratio\n",
    "\n",
    "    @staticmethod\n",
    "    def check_same_class_labels(labels):\n",
    "        \"\"\"\n",
    "            Checks set of labels to ensure they are of the same class type\n",
    "        :param labels:\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        if len(set(labels)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def attribute_selection_method(self, D, attribute_list):\n",
    "        \"\"\"\n",
    "            Attribute Selection Method for decision tree as discussed in [1] (Figure 8.3), selects attribute that\n",
    "            provides the best information gain ratio as a result.\n",
    "        :param D:\n",
    "        :param attribute_list:\n",
    "        :return: best_attribute\n",
    "        \"\"\"\n",
    "        best_attribute = ''\n",
    "        dataset_entropy = self.data_entropy(D[1])\n",
    "        best_info_gain_ratio = 0.0\n",
    "        split_val = ''\n",
    "\n",
    "        for attribute in attribute_list:\n",
    "            # a_idx = self.attributes.get(attribute) MIGHT NEED THIS\n",
    "            v = D[0][attribute].unique()  # find v distinct values of attribute\n",
    "            att_ent = 0.0\n",
    "            split_info = 0.0\n",
    "            curr_val = ''\n",
    "            val_ent = 0.0\n",
    "            for val in v:\n",
    "                data_partition = self.partition_data(D, attribute, val)\n",
    "                partition_labels = data_partition[1]\n",
    "                part_entropy = self.data_entropy(partition_labels)\n",
    "                p_j = float(len(data_partition[1]) / len(D[1]))\n",
    "                att_ent = att_ent + (p_j * part_entropy)\n",
    "                split_info = split_info - self.split_info(p_j)\n",
    "\n",
    "                if part_entropy > val_ent:\n",
    "                    val_ent = part_entropy\n",
    "                    curr_val = val\n",
    "\n",
    "            # Best Attribute checks\n",
    "            if split_info == 0:  # prevent division by zero for ratio\n",
    "                continue\n",
    "            else:\n",
    "                info_gain = self.information_gain(dataset_entropy, att_ent)\n",
    "                info_gain_ratio = self.information_gain_ratio(info_gain,\n",
    "                                                              split_info)  # calculate info gain ratio to select\n",
    "\n",
    "            # compare the top performing attribute info gain ratio\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_attribute = attribute\n",
    "                split_val = curr_val\n",
    "        return best_attribute, split_val\n",
    "\n",
    "    def class_prob(self, feature_label, labels):\n",
    "        \"\"\"\n",
    "            Computes class probabilities from labels\n",
    "        :param feature_label:\n",
    "        :param labels:\n",
    "        :return: p\n",
    "        \"\"\"\n",
    "        c = collections.Counter(labels)  # [4]\n",
    "        p = c[feature_label] / len(labels)\n",
    "        return float(p)\n",
    "\n",
    "    def data_entropy(self, labels):\n",
    "        \"\"\"\n",
    "            Computes the Entropy, or Info(D) [1]\n",
    "        :param labels:\n",
    "        :return: entropy\n",
    "        \"\"\"\n",
    "        entropy = 0.0\n",
    "        class_freq = collections.Counter(labels)  # [4]\n",
    "        for l in class_freq.keys():\n",
    "            p = float(class_freq[l] / len(labels))\n",
    "            entropy = entropy - math.log(p, 2)\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, dataset_entropy, attribute_entropy):\n",
    "        \"\"\"\n",
    "            Computes information gain based on the data entropy and attribute entropy [1]\n",
    "        :param dataset_entropy:\n",
    "        :param attribute_entropy:\n",
    "        :return: gain\n",
    "        \"\"\"\n",
    "        gain = dataset_entropy - attribute_entropy\n",
    "        return gain\n",
    "\n",
    "    def split_info(self, p_j):\n",
    "        \"\"\"\n",
    "            Computes the information split, used in gain ratio [1]\n",
    "        :param p_j:\n",
    "        :return: info_split\n",
    "        \"\"\"\n",
    "        # error protection for zero case\n",
    "        if p_j == 0:\n",
    "            return 0\n",
    "\n",
    "        info_split = (p_j * math.log(p_j, 2))\n",
    "        return info_split\n",
    "\n",
    "    def information_gain_ratio(self, gain, split_info):\n",
    "        \"\"\"\n",
    "            Computes information gain ratio [1]\n",
    "        :param gain:\n",
    "        :param split_info:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gain_ratio = float(gain / split_info)\n",
    "        return gain_ratio\n",
    "\n",
    "    def partition_data(self, D, attribute, val):\n",
    "        \"\"\"\n",
    "            Partitions a dataset D based on the value of a specific attribute\n",
    "        :param D:\n",
    "        :param attribute:\n",
    "        :param val:\n",
    "        :return: part, part_y\n",
    "        \"\"\"\n",
    "        part = D[0].loc[D[0][attribute] == val]\n",
    "        part_idx = D[0].index[D[0][attribute] == val]\n",
    "        part_y = D[1].loc[part_idx]\n",
    "        return part, part_y\n",
    "\n",
    "    def test_tree(self, test_sample, node):\n",
    "        \"\"\"\n",
    "            Using recursion, we go through each node (from the root through to the children) to find a leaf label\n",
    "            to classify the test sample as a prediction.\n",
    "        :param test_sample:\n",
    "        :param node:\n",
    "        :return: node.leaf_label, or recursion\n",
    "        \"\"\"\n",
    "\n",
    "        if node.node_type == 'leaf':\n",
    "            return node.leaf_label\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                if (child.best_attribute is None or child.best_attribute == '') and child.node_type == 'leaf':\n",
    "                    return self.test_tree(test_sample, child)\n",
    "\n",
    "                if (child.best_attribute is None or child.best_attribute == '') and child.node_type == 'node':\n",
    "                    pass\n",
    "                else:\n",
    "                    if child.split_criterion == test_sample[child.best_attribute]:\n",
    "                        return self.test_tree(test_sample, child)\n",
    "                    else:\n",
    "                        if child.split_up_down == 'UP':\n",
    "                            # check if att_val > split_criterion\n",
    "                            if pd.to_numeric(test_sample[child.best_attribute]) > float(child.split_criterion):\n",
    "                                return self.test_tree(test_sample, child)\n",
    "                            else:\n",
    "                                pass\n",
    "                        elif child.split_up_down == 'DOWN':\n",
    "                            if pd.to_numeric(test_sample[child.best_attribute]) <= float(child.split_criterion):\n",
    "                                return self.test_tree(test_sample, child)\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "    def predict(self, test_x, test_y):  # TODO Add this functionality from the code in main routine\n",
    "        # uses test set to predict class labels from the constructed tree\n",
    "        preds = []\n",
    "        true_pred = 0\n",
    "        for i in range(len(test_x)):\n",
    "            tester_instance = test_x.iloc[i]\n",
    "            pred = self.test_tree(tester_instance, self.root_node)\n",
    "            # print(str(i), 'pred', pred, 'label', y.iloc[i])\n",
    "            if pred == test_y.iloc[i]:\n",
    "                true_pred += 1\n",
    "            preds.append(pred)\n",
    "\n",
    "        return true_pred, preds\n",
    "\n",
    "    def print_tree(self):\n",
    "        nodes_created = sorted(self.tree_nodes)\n",
    "        for n in nodes_created:\n",
    "            n.print_node()\n",
    "            for d in n.children:\n",
    "                d.print_node()\n",
    "            print()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>0.240810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "      <td>0.427581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  education-num  capital-gain  capital-loss  \\\n",
       "count  32561.000000   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647      10.080679   1077.648844     87.303830   \n",
       "std       13.640433       2.572720   7385.292085    402.960219   \n",
       "min       17.000000       1.000000      0.000000      0.000000   \n",
       "25%       28.000000       9.000000      0.000000      0.000000   \n",
       "50%       37.000000      10.000000      0.000000      0.000000   \n",
       "75%       48.000000      12.000000      0.000000      0.000000   \n",
       "max       90.000000      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week        income  \n",
       "count    32561.000000  32561.000000  \n",
       "mean        40.437456      0.240810  \n",
       "std         12.347429      0.427581  \n",
       "min          1.000000      0.000000  \n",
       "25%         40.000000      0.000000  \n",
       "50%         40.000000      0.000000  \n",
       "75%         45.000000      0.000000  \n",
       "max         99.000000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train_data = pd.read_csv('adult.data', header= None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "test_data = pd.read_csv('adult.test', header= None, skiprows=1,  names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "od = test_data\n",
    "\n",
    "#編號與特徵無關\n",
    "train_data = train_data.drop(['fnlwgt'], axis=1)\n",
    "test_data = test_data.drop(['fnlwgt'], axis=1)\n",
    "\n",
    "#education與education-num相對應(重複)，故刪除。\n",
    "train_data.drop(['education'], axis = 1, inplace = True)\n",
    "test_data.drop(['education'], axis = 1, inplace = True)\n",
    "\n",
    "# 每個元素的前後空格去掉\n",
    "train_data = train_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "test_data = test_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "#查看各欄\"?\"各數\n",
    "# train_data.apply(lambda x: np.sum(x == \"?\"))\n",
    "# test_data.apply(lambda x: np.sum(x == \"?\"))\n",
    "\n",
    "#把\"?\"取代為NaT\n",
    "train_data.replace(\"?\", pd.NaT, inplace = True)\n",
    "test_data.replace(\"?\", pd.NaT, inplace = True)\n",
    "\n",
    "\n",
    "# #將缺失值補齊，名目資料填眾數，數值資料填平均值。\n",
    "fill_data = {'workclass': train_data['workclass'].mode()[0], 'occupation': train_data['occupation'].mode()[0], 'native-country': train_data['native-country'].mode()[0]}\n",
    "fill_data_test = {'workclass': test_data['workclass'].mode()[0], 'occupation': test_data['occupation'].mode()[0], 'native-country': test_data['native-country'].mode()[0]}\n",
    "\n",
    "#缺失值填充\n",
    "train_data.fillna(fill_data, inplace=True)\n",
    "test_data.fillna(fill_data_test, inplace=True)\n",
    "\n",
    "#將income欄位>50K改為1，<=50K改為0\n",
    "train_data['income'] = train_data['income'].apply(lambda x: 0 if x == \"<=50K\" else 1)\n",
    "test_data['income'] = test_data['income'].apply(lambda x: 0 if x == '<=50K.' else 1)\n",
    "\n",
    "\n",
    "train_data.describe()\n",
    "# train_data.info()\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預處理_正規化、Deal with duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Extract numerical columns\n",
    "numerical_columns = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data[numerical_columns]), columns=numerical_columns)\n",
    "train_data[numerical_columns] = train_data_scaled\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "test_data_scaled = pd.DataFrame(scaler.transform(test_data[numerical_columns]), columns=numerical_columns)\n",
    "test_data[numerical_columns] = test_data_scaled\n",
    "\n",
    "\n",
    "# 使用pandas的get_dummies函數執行獨熱編碼\n",
    "train_data = pd.get_dummies(train_data, columns=['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'], dtype=int)\n",
    "\n",
    "\n",
    "test_data = pd.get_dummies(test_data, columns=['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'], dtype=int)\n",
    "\n",
    "#在讀熱編碼後會依照有名目之欄位產生資料，train_data比test_data多出了該欄位，故將test_data新增該欄位，讓兩個資料集欄位相同。\n",
    "test_data['native-country_Holand-Netherlands'] = 0\n",
    "\n",
    "#刪除重複列\n",
    "train_data.drop_duplicates(inplace=True)\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "\n",
    "#將資料隨機化\n",
    "from sklearn.utils import shuffle\n",
    "train_data = shuffle(train_data)\n",
    "\n",
    "#train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試集準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full set test accuracy: 0.814089505158704\n"
     ]
    }
   ],
   "source": [
    "td = train_data\n",
    "column = td.pop(\"income\")\n",
    "td.insert(td.shape[1], \"income\", column)\n",
    "####\n",
    "\n",
    "columns = td.columns.tolist()\n",
    "# print(columns)\n",
    "\n",
    "X_train = td.drop('income', axis=1)\n",
    "y_train = td['income']\n",
    "\n",
    "ted = test_data\n",
    "t_column = ted.pop(\"income\")\n",
    "ted.insert(ted.shape[1], \"income\", t_column)\n",
    "\n",
    "\n",
    "X_test = ted.drop('income', axis=1)\n",
    "y_test = ted['income']\n",
    "\n",
    "\n",
    "\n",
    "system_test = C45Tree(columns, td)\n",
    "system_test.train(X_train, y_train)\n",
    "true_pred, preds = system_test.predict(X_test, y_test)\n",
    "\n",
    "print('Full set test accuracy:', true_pred / len(X_test))\n",
    "\n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預測結果轉換為 Python 陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將 R 的預測結果轉換為 Python 陣列\n",
    "#rpy2\n",
    "from openpyxl import Workbook\n",
    "#產出Excel(Test data)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append(['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income','Predict result'])\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == 0:\n",
    "        result = '<=50K.'\n",
    "    else:\n",
    "        result = '>50K.'\n",
    "    #將現在loop到原始資料的列轉為list\n",
    "    li = od.iloc[i,:].tolist()\n",
    "    #\n",
    "    li.append(result)\n",
    "    ws.append(li)\n",
    "wb.save('C45.xlsx')\n",
    "# predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "節點數!\n"
     ]
    }
   ],
   "source": [
    "print(\"節點數!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "print(len(system_test.tree_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "葉子!\n"
     ]
    }
   ],
   "source": [
    "print(\"葉子!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算葉節點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "leaf_count = 0\n",
    "for n in system_test.tree_nodes:\n",
    "    # print(n.print_node())\n",
    "    if n.node_type == 'leaf':\n",
    "        leaf_count += 1\n",
    "\n",
    "print(leaf_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
